\documentclass[a4paper, 11pt, landscape]{article}

\usepackage{mathptmx} % more compact font
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{multicol}
\usepackage{enumitem}
\usepackage{paralist} % for compacter lists
\usepackage{hyperref} % for Todo's and similar things
\usepackage[left=4.5mm, right=4.5mm, top=4.5mm, bottom=6mm, landscape, nohead, nofoot]{geometry}
\usepackage[small,compact]{titlesec}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{xparse}
\usepackage{bm}

% colored matrices / text in color
\usepackage[dvipsnames]{xcolor}
% \colorlet{LightRubineRed}{RubineRed!70!}
% \colorlet{Mycolor1}{green!10!orange!90!}
% \definecolor{Mycolor2}{HTML}{00F9DE}
\definecolor{matrixcolor}{HTML}{04940A}
\definecolor{bulletcolor}{HTML}{BF0052}
\newcommand{\red}{\textcolor{matrixcolor}}

% compact text
\linespread{0.9}
\setlength{\parindent}{0pt}

% compact lists even more
\setdefaultleftmargin{0em}{0em}{0em}{0em}{0em}{0em}

% compact sections
\titlespacing*{\section}{0pt}{0em}{0em}
\titlespacing*{\subsection}{0pt}{0em}{0em}
\titlespacing*{\subsubsection}{0pt}{0em}{0em}

% coloured section headings for easier read
\titleformat{name=\section}[block]
{\sffamily}
{}
{0pt}
{\colorsection}
\newcommand{\colorsection}[1]{%
	\colorbox{RoyalBlue!80!Black!90}{\parbox[t][0em]{\dimexpr\columnwidth-2\fboxsep}{\textcolor{white}{\textsc{\thesection\ #1}}}}}


\titleformat{name=\subsection}[block]
{\sffamily}
{}
{0pt}
{\subcolorsection}
\newcommand{\subcolorsection}[1]{%
	\colorbox{RoyalBlue!50}{\parbox[t][0em]{\dimexpr\columnwidth-2\fboxsep}{\thesubsection\ #1}}}


\titleformat{name=\subsubsection}[block]
{\sffamily}
{}
{0pt}
{\subsubcolorsection}
\newcommand{\subsubcolorsection}[1]{%
	\colorbox{RoyalBlue!20}{\parbox[t][0em]{\dimexpr\columnwidth-2\fboxsep}{\thesubsubsection\ #1}}}

% environment for multicols inside a list
\NewDocumentEnvironment{listcols}{O{2} O{0pt}}
	{%
		\bgroup %
		\setlength{\multicolsep}{#2} %
		\begin{multicols*}{#1} %
	}
	{%
		\end{multicols*} %
		\egroup %
	}

% multicols lines & spacing
\setlength{\columnsep}{0.15cm}
\setlength{\columnseprule}{0.15pt}

% No page numbers
\pagenumbering{gobble}

% math helpers
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}
\begin{multicols*}{3}


\section{Essentials}
\begin{compactdesc}
	\item[Inner Product:] $\langle \mathbf{\red{\red{x}}}, \mathbf{\red{y}} \rangle = \mathbf{\red{x}}^\top \mathbf{\red{y}} = \sum_{i=1}^{N} \mathbf{\red{x}}_i \mathbf{\red{y}}_i$
	\begin{inparaitem}[\color{bulletcolor}\textbullet]
		\item $\langle \mathbf{\red{x}}, \mathbf{\red{y+z}} \rangle = \langle \mathbf{\red{x}}, \mathbf{\red{y}} \rangle + \langle \mathbf{\red{x}}, \mathbf{\red{z}} \rangle = \langle \mathbf{\red{x+y}}, \mathbf{\red{z}} \rangle $
	\end{inparaitem}
	\begin{inparaitem}[\color{bulletcolor}\textbullet]
		\item $\langle \alpha \mathbf{\red{u}}, \mathbf{\red{a}} \rangle = \alpha \langle \mathbf{\red{u}}, \mathbf{\red{a}} \rangle$
	\end{inparaitem}
	\item[orthogonal:] $\mathbf{\red{\red{A^{-1}}}} = \mathbf{\red{A^{T}}}$
	\begin{inparaitem}[\color{bulletcolor}\textbullet]
		\item $det(\mathbf{\red{A}}) \in +1/-1$
		\item $det(\mathbf{\red{AA^{T}}}) = 1$
		\item $\mathbf{\red{U_K U_K^\top U}} = [\mathbf{\red{U_K}};\mathbf{\red{0}}]$
		\item $\mathbf{\red{U}}: orth. \rightarrow \mathbf{\red{U^\top}} orth.$
		\item $\|\mathbf{\red{Ux}}\|_2 = \|\mathbf{\red{x}}\|_2$
		\item $\|\mathbf{\red{A}}\|_2 = \max_{\mathbf{\red{x}}} \|\mathbf{\red{Ax}}\|_2 = \max_{\mathbf{\red{x}}} \|\mathbf{\red{U(DV^\top x)}}\|_2 = \max_{\mathbf{\red{x}}} \|\mathbf{\red{DV^\top x}}\|_2 = \max_{\mathbf{\red{y}}} \|\mathbf{\red{Dy}}\|_2 = \|\mathbf{\red{D}}\|_2$ with $\|\mathbf{\red{x}}\|_2 = 1$ and $\mathbf{\red{y}}=\mathbf{\red{V^\top x}}$
		\item $\langle \mathbf{\red{\red{u}}}, \mathbf{\red{v}} \rangle = 0$
	\end{inparaitem}
	\item[trace:] $\mathrm{tr}(\mathbf{\red{XYZ}}) = \mathrm{tr}(\mathbf{\red{ZXY}})$
	\begin{inparaitem}[\color{bulletcolor}\textbullet]
		\item $\mathrm{tr}(c \mathbf{\red{A}}) = c * \mathrm{tr}(\mathbf{\red{A}})$ 
		\item $\mathrm{tr}(\mathbf{\red{A}}+\mathbf{\red{B}}) = \mathrm{tr}(\mathbf{\red{A}}) + \mathrm{tr}(\mathbf{\red{B}})$
		\item $\mathrm{tr}(\mathbf{\red{u_i u_i^\top}}) = 1$
	\end{inparaitem}
	\item[CoordDesc:] $f(\mathbf{\red{x}}) = \frac{1}{2} \|\mathbf{\red{y}}-\mathbf{\red{Ax}}\|^2$: $\nabla_i f(\mathbf{\red{x}}) = \frac{1}{2} \frac{\partial}{\partial \mathbf{\red{x}}_i} \|\mathbf{\red{y}}-\mathbf{\red{Ax}}\|^2 = -2\frac{1}{2}[\frac{\partial}{\partial \mathbf{\red{x}}_i} \mathbf{\red{Ax}}]^\top (\mathbf{\red{y}}-\mathbf{\red{Ax}}) \stackrel{1}{=} \mathbf{\red{A_i}}^\top(\mathbf{\red{Ax}}-\mathbf{\red{y}}) \stackrel{2}{=} \mathbf{\red{A_i}}^\top(\mathbf{\red{A_{i}x_i}} + \sum_{j \neq i}\mathbf{\red{A_{j}x_j}}-\mathbf{\red{y}}) \stackrel{!}{=} 0 \rightarrow \mathbf{\red{x_i}} = \frac {\mathbf{\red{A_i}}^\top(\mathbf{\red{y}}-\sum_{j \neq i}\mathbf{\red{A_{j}x_j}})}{\mathbf{\red{A_i}}^\top \mathbf{\red{A_i}}}$
	\begin{inparaenum}[\color{bulletcolor} 1.]
		\item $\frac{\partial}{\partial \mathbf{\red{x}}_i} \mathbf{\red{Ax}} = \frac{\partial}{\partial \mathbf{\red{x}}_i} \sum_i \mathbf{\red{A_{i}x_i}} = \mathbf{\red{A_i}}$
		\item $\mathbf{\red{Ax}} = \sum_{j}\mathbf{\red{A_{j}x_j}} = \mathbf{\red{A_{i}x_i}} \sum_{j \neq i} \mathbf{\red{A_{j}x_j}}$
	\end{inparaenum}
	\item[EVD:] $\mathbf{\red{Y}} = \mathbf{\red{X}} + \mathbf{\red{uu^\top}}$
	\begin{inparaitem}[\color{bulletcolor}\textbullet]
		\item $\mathbf{\red{X}}$:symm 
		\item distinct $\lambda_1$ \& $\lambda_2$
		\item EV $\mathbf{\red{u}}$ \& $\mathbf{\red{v}}$
		\item $\mathbf{\red{Y}}$ symm: $\mathbf{\red{Y}}^\top = \mathbf{\red{Y}}$
		\item $\mathbf{\red{u}}$ EV of $\mathbf{\red{Y}}$: $\mathbf{\red{Yu}} = (\mathbf{\red{X}} + \mathbf{\red{uu^\top}})\mathbf{\red{u}} = \mathbf{\red{Xu}} + \mathbf{\red{uu^\top u}} \stackrel{\mathbf{\red{uu^\top}} = 1}{=} \mathbf{\red{Xu}} + \mathbf{\red{u}} \stackrel{\mathbf{\red{Xu}} = \lambda_1 \mathbf{\red{u}}}{=} \lambda_1 \mathbf{\red{u}} + \mathbf{\red{u}} = (\lambda_1 +1)\mathbf{\red{u}}$
		\item $\mathbf{\red{v}}$ EV of $\mathbf{\red{Y}}$: $\mathbf{\red{Yv}} = (\mathbf{\red{X}} + \mathbf{\red{uu^\top}})\mathbf{\red{v}} = \mathbf{\red{Xv}} + \mathbf{\red{uu^\top v}} \stackrel{\mathbf{\red{u^\top v}}=0}{=} \mathbf{\red{Xv}} = \lambda_2 \mathbf{\red{v}}$
	\end{inparaitem}
\end{compactdesc}



\subsection{Norms}
\begin{inparaitem}[\color{bulletcolor}\textbullet]
	\item $\|\mathbf{\red{x}}\|_0 := |\{i | x_i \neq 0\}|$
	\item $\|\mathbf{\red{x}}\|_2 := \sqrt{\sum_{i=1}^{N} \mathbf{\red{x}}_i^2} = \sqrt{\mathbf{\red{x}}^\top \mathbf{\red{x}}}$
	\item $\|\mathbf{\red{x}}\|_p := \left( \sum_{i=1}^{N} |x_i|^p \right)^{\frac{1}{p}}$
	\item $\|\mathbf{\red{X}}\|_\star = \sum_{i=1}^{\min(m, n)} \sigma_i$
	\item $\|\mathbf{\red{A}}\|_F :=\allowbreak \sqrt{\sum_{i=1}^{M} \sum_{j=1}^{N} |\mathbf{\red{A}}_{i, j}|^2} =\allowbreak \sqrt{\operatorname{trace}(\mathbf{\red{A}}^\top \mathbf{\red{A}})} =\allowbreak \sqrt{\sum_{i=1}^{\min\{m, n\}} \sigma_i^2}$
	\item $\|\mathbf{\red{X}}\|_1 = \max_{1 \leq j \leq n} \sum_{i=1}^{m}|x_{ij}|$
    \item $\|\mathbf{\red{X}}\|_2 = \sigma_{max}(\mathbf{\red{X}})$
\end{inparaitem}

\subsection{Derivatives}
\begin{compactdesc}
	\item[$\partial f/\partial \mathbf{\red{X}}$:]
\begin{inparaitem}[\color{bulletcolor}\textbullet]
  \item $\partial \mathbf{\red{A}} = 0$
  \item $\partial(\alpha \mathbf{\red{X}}) = \alpha \partial \mathbf{\red{X}}$
  \item $\partial(\mathbf{\red{X}}+\mathbf{\red{Y}}) = \partial(\mathbf{\red{X}}) + \partial(\mathbf{\red{Y}})$
  \item $\partial(trace(\mathbf{\red{X}})) = trace(\partial \mathbf{\red{X}})$
  \item $\partial(\mathbf{\red{X}}\mathbf{\red{Y}}) = \partial(\mathbf{\red{X}})\mathbf{\red{Y}} +\mathbf{\red{X}}\partial(\mathbf{\red{Y}})$
  \item $\partial(\mathbf{\red{X^{-1}}}) = -\mathbf{\red{X^{-1}}} (\partial\mathbf{\red{X}}) \mathbf{\red{X^{-1}}}$
  \item $\partial \mathbf{\red{X^{T}}} = (\partial \mathbf{\red{X}})^{T}$
  \end{inparaitem}
  \item[Vectors:]
  \begin{inparaitem}[\color{bulletcolor}\textbullet]
  \item $\frac{\partial}{\partial \mathbf{\red{x}}}(\mathbf{\red{b}}^\top \mathbf{\red{x}}) = \frac{\partial}{\partial \mathbf{\red{x}}}(\mathbf{\red{x}}^\top \mathbf{\red{b}}) = \mathbf{\red{b}}$
  \item $\frac{\partial}{\partial \mathbf{\red{x}}}(\mathbf{\red{x}}^\top \mathbf{\red{x}}) = 2\mathbf{\red{x}}$
    \item $\frac{\partial}{\partial \mathbf{\red{x}}}(\mathbf{\red{A}} \mathbf{\red{x}}) = \mathbf{\red{A}}$
  \item $\frac{\partial}{\partial \mathbf{\red{x}}}(\mathbf{\red{x}}^\top \mathbf{\red{A}}\mathbf{\red{x}}) = (\mathbf{\red{A}}^\top + \mathbf{\red{A}})\mathbf{\red{x}} =^{\text{if \textbf{A} sym.}} 2\mathbf{\red{A}}\mathbf{\red{x}}$
  \item $\frac{\partial}{\partial \mathbf{\red{x}}}(\mathbf{\red{b}}^\top \mathbf{\red{A}}\mathbf{\red{x}}) = \mathbf{\red{A}}^\top \mathbf{\red{b}}$
  \item $\frac{\partial}{\partial \mathbf{\red{x}}}(\mathbf{\red{s}}^\top \mathbf{\red{A}} \mathbf{\red{r}}) = (\frac{\partial \mathbf{\red{s}}}{\partial \mathbf{\red{x}}})^\top \mathbf{\red{A}} \mathbf{\red{r}} + (\frac{\partial \mathbf{\red{r}}}{\partial \mathbf{\red{x}}})^\top \mathbf{\red{A}}^\top \mathbf{\red{s}}$
  \end{inparaitem}
  \item[scalar $\alpha$:]
  \begin{inparaitem}[\color{bulletcolor}\textbullet]
  	\item $\frac{\partial}{\partial \mathbf{\red{x}}}(\mathbf{\red{y}}^\top \mathbf{\red{A}} \mathbf{\red{x}}) = \mathbf{\red{y}}^\top \mathbf{\red{A}}$
  	\item $\frac{\partial}{\partial \mathbf{\red{y}}}(\mathbf{\red{y}}^\top \mathbf{\red{A}} \mathbf{\red{x}}) = \mathbf{\red{x}}^\top \mathbf{\red{A}}^\top$
  \end{inparaitem}
  \item[Matrices:]
  \begin{inparaitem}[\color{bulletcolor}\textbullet]
  \item $\frac{\partial}{\partial \mathbf{\red{X}}}(\mathbf{\red{b}}^\top \mathbf{\red{X}}^\top \mathbf{\red{X}} \mathbf{\red{c}}) = \mathbf{\red{X}} (\mathbf{\red{b}}\mathbf{\red{c}}^\top + \mathbf{\red{c}}\mathbf{\red{b}}^\top)$
  \item $\frac{\partial}{\partial \mathbf{\red{X}}}(\mathbf{\red{c}}^\top \mathbf{\red{X}} \mathbf{\red{b}}) = \mathbf{\red{c}}\mathbf{\red{b}}^\top$
  \item $\frac{\partial}{\partial \mathbf{\red{X}}}(\mathbf{\red{c}}^\top \mathbf{\red{X}}^\top \mathbf{\red{b}}) = \mathbf{\red{b}}\mathbf{\red{c}}^\top$
  \item $\frac{\partial}{\partial \mathbf{\red{U}}}(\mathbf{\red{U^\top V}}) = \frac{\partial}{\partial \mathbf{\red{U}}}(\mathbf{\red{V^\top U}})^\top = (\frac{\partial}{\partial \mathbf{\red{U}}}\mathbf{\red{V^\top U}})^\top = (\mathbf{\red{V^\top}})^\top = \mathbf{\red{V}}$
  \item $\frac{\partial f}{\partial \mathbf{\red{X}}^\top} = (\frac{\partial f}{\partial \mathbf{\red{X}}})^\top$
  \end{inparaitem}
  \item[Norms:]
  \begin{inparaitem}[\color{bulletcolor}\textbullet]
  \item $\frac{\partial}{\partial \mathbf{\red{x}}}(\| \mathbf{\red{x}} \|_1) = \mathbf{\red{1}}$
  \item $\frac{\partial}{\partial \mathbf{\red{x}}}(\| \mathbf{\red{x}}-\mathbf{\red{b}} \|_2) = \frac{\mathbf{\red{x}}-\mathbf{\red{b}}}{\|\mathbf{\red{x}}-\mathbf{\red{b}}\|_2}$
  \item $\frac{\partial}{\partial \mathbf{\red{x}}}(\|\mathbf{\red{x}}\|^2_2) = \frac{\partial}{\partial \mathbf{\red{x}}} (\|\mathbf{\red{x}}^\top \mathbf{\red{x}}\|_2) = 2\mathbf{\red{x}}$
  \item $\frac{\partial}{\partial \mathbf{\red{X}}}(\|\mathbf{\red{X}}\|_F^2) = 2\mathbf{\red{X}}$
  \item $\frac{\partial}{\partial \mathbf{\red{W}}}(\|\mathbf{\red{XW}}-\mathbf{\red{Y}}\|_F^2) = 2 \mathbf{\red{X}}^\top (\mathbf{\red{XW}} - \mathbf{\red{Y}})$
  \end{inparaitem}
\end{compactdesc}

\subsection{Eigendecomposition}
\begin{inparaitem}[\color{bulletcolor}\textbullet]
  \item $\mathbf{\red{A}} \in \mathbb{R}^{N \times N}$: $\mathbf{\red{A}} = \mathbf{\red{Q}} \red{\boldsymbol{\Lambda}} \mathbf{\red{Q}}^{-1}$ \& $\mathbf{\red{Q}} \in \mathbb{R}^{N \times N}$
  \item if all $\lambda_i \neq 0$: $\mathbf{\red{A}}^{-1} = \mathbf{\red{Q}} \red{\boldsymbol{\Lambda}}^{-1} \mathbf{\red{Q}}^{-1}$ and $(\red{\boldsymbol{\Lambda}}^{-1})_{i,i} = \frac{1}{\lambda_i}$
  \item if $\mathbf{\red{A}}$ symm.: $A = \mathbf{\red{Q}} \red{\boldsymbol{\Lambda}} \mathbf{\red{Q^\top}}$
\end{inparaitem}

\subsection{Probability / Statistics}
\begin{inparaitem}[\color{bulletcolor}\textbullet]
	\item $P(x) := Pr[X = x] := \sum_{y \in Y} P(x, y)$
	\item $P(x|y) := Pr[X = x | Y = y] := \frac{P(x,y)}{P(y)},\quad \text{if } P(y) > 0$
	\item $\forall fixed y \in Y: \sum_{x \in X} P(x|y) = 1$
	\item $P(x, y) = P(x|y) P(y)$
	\item $P(x|y) = \frac{P(y|x)P(x)}{P(y)}$
	\item $P(x|y) = P(x) \Leftrightarrow P(y|x) = P(y)$ (iff $X$, $Y$ ind.)
	\item $P(x_1, \ldots, x_n) \stackrel{IID}{=} \prod_{i=1}^n P(x_i)$
\end{inparaitem}


\section{Dimensionality Reduction / PCA}
$\mathbf{\red{X}} \in \mathbb{R}^{D \times N}$. $N$ observations, $K$ properties. Target: $\tilde{\mathbf{\red{X}}} \in \mathbb{R}^{K \times N}$.
\begin{inparaenum}[\color{bulletcolor} 1.]
	\item Mean: $\overline{\mathbf{\red{x}}} = \frac{1}{N} \sum_{n=1}^N \mathbf{\red{x}}_n$
	\item Center: $\overline{\mathbf{\red{X}}} = \mathbf{\red{X}} - [\overline{\mathbf{\red{x}}}, \ldots, \overline{\mathbf{\red{x}}}] = \mathbf{\red{X}} - \mathbf{\red{M}}$
	\item Cov. Matrix: $\red{\boldsymbol{\Sigma}} = \frac{1}{N	} \sum_{n=1}^N (\mathbf{\red{x}}_n - \overline{\mathbf{\red{x}}}) (\mathbf{\red{x}}_n - \overline{\mathbf{\red{x}}})^\top = \frac{1}{N} \overline{\mathbf{\red{X}}}\overline{\mathbf{\red{X}}}^\top$
	\item EVD: $\red{\boldsymbol{\Sigma}} = \mathbf{\red{U}} \red{\boldsymbol{\Lambda}} \mathbf{\red{U}}^\top$
	\item Select $K < D$, $\Rightarrow \mathbf{\red{U}}_K, \red{\boldsymbol{\lambda}}_K$
	\item Transform to new Basis: $\overline{\mathbf{\red{Z}}}_K = \mathbf{\red{U}}_K^\top \overline{\mathbf{\red{X}}}$
	\item Reconstruct original Basis: $\tilde{\overline{\mathbf{\red{X}}}} = \mathbf{\red{U}}_k \overline{\mathbf{\red{Z}}}_K$
	\item Reverse centering: $\tilde{\mathbf{\red{X}}} = \tilde{\overline{\mathbf{\red{X}}}} + \mathbf{\red{M}}$
\end{inparaenum}

\begin{compactitem}
	\item $\mathbf{\red{U}}_k \in \mathbb{R}^{D \times K}, \red{\boldsymbol{\Sigma}} \in \mathbb{R}^{D \times D}, \overline{\mathbf{\red{Z}}}_K \in \mathbb{R}^{K \times N}, \overline{\mathbf{\red{X}}} \in \mathbb{R}^{D \times N}$
	\item error $J = \sum^D_{d=K+1} \mathbf{\red{u}}^\top_d \red{\boldsymbol{\Sigma}} \mathbf{\red{u}}_d$
\end{compactitem}

\section{SVD}
\begin{inparaitem}[\color{bulletcolor}\textbullet]
	\item $\mathbf{\red{A}} = \mathbf{\red{U}} \mathbf{\red{D}} \mathbf{\red{V}}^\top = \sum_{k=1}^{\operatorname{rank}(\mathbf{\red{A}})} d_{k,k} u_k (v_k)^\top$
	\item $\mathbf{\red{A}} \in \mathbb{R}^{N \times P}, \mathbf{\red{U}} \in \mathbb{R}^{N \times N}, \mathbf{\red{D}} \in \mathbb{R}^{N \times P}, \mathbf{\red{V}} \in \mathbb{R}^{P \times P}$
	\item $\mathbf{\red{U}}^\top \mathbf{\red{U}} = I = \mathbf{\red{V}}^\top \mathbf{\red{V}}$
	\item $\mathbf{\red{U}}$ columns are EVs of $\mathbf{\red{A}} \mathbf{\red{A}}^\top$, $\mathbf{\red{V}}$ columns are EVs of $\mathbf{\red{A}}^\top \mathbf{\red{A}}$, $\sigma = \sqrt{\lambda}$. If $\mathbf{\red{S}} = \mathbf{\red{S}}^\top \Rightarrow \mathbf{\red{S}} = \mathbf{\red{U}}\mathbf{\red{D}}\mathbf{\red{U}}^\top$
	\item Missing columns in $\mathbf{\red{U}}$ are basis of $\operatorname{null}(A^\top)$ and in $\mathbf{\red{V}}$ are basis of $\operatorname{null}(A)$. 
	\item $\mathbf{\red{U}}$: users-to-concept, $\mathbf{\red{V}}$: Movies-to-concept, $\mathbf{\red{D}}$: expressiveness of concept
\end{inparaitem}

\subsection{Low-Rank approximation}
$\tilde{\mathbf{\red{A}}}_{i, j} = \sum_{k=1}^K \mathbf{\red{U}}_{i, k} \mathbf{\red{D}}_{k,k} \mathbf{\red{V}}_{j, k} = \mathbf{\red{U}}_{i, k} \mathbf{\red{D}}_{k,k} (\mathbf{\red{V}}^\top)_{k, j}$.
\begin{compactdesc}
	\item[Error Frobenius:] $\|\mathbf{\red{A}} - \tilde{\mathbf{\red{A}}}\|_F = \sqrt{\sum_{i > K} \sigma_i^2} = \sqrt{\sum_{i > K} \lambda_i}$
	\item[Error Euclidean:] $\|\mathbf{\red{A}} - \tilde{\mathbf{\red{A}}}\|_2 = \sigma_{K+1}$
\end{compactdesc}

\section{$K$-means Algorithm}
\begin{inparadesc}
	\item[\color{bulletcolor}Target:] $\min_{\mathbf{\red{U}}, \mathbf{\red{Z}}} J(\mathbf{\red{U}}, \mathbf{\red{Z}}) = \|\mathbf{\red{X}} - \mathbf{\red{U}} \mathbf{\red{Z}}\|_F^2 = \sum_{n=1}^N \sum_{k=1}^K z_{k,n} \|\mathbf{\red{x}}_n - \mathbf{\red{u}}_k\|_2^2$
\end{inparadesc}
\begin{inparaenum}[\color{bulletcolor} 1.]
	\item choose $K$ centroids $\mathbf{\red{U}} = [\mathbf{\red{u}}_1, \ldots, \mathbf{\red{u}}_K]$
	\item Assign data points to clusters. $k^\star(\mathbf{\red{x}}_n) = \argmin_k \{ \|\mathbf{\red{x}}_n - \mathbf{\red{u}}_k\|_2 \}$. Set $\mathbf{\red{z}}_{k^\star,n} = 1$, and for $ l \neq k^\star~ \mathbf{\red{z}}_{l,n}=0$.
	\item Update centroids: $\mathbf{\red{u}}_k = \frac{\sum_{n=1}^N z_{k,n} \mathbf{\red{x}}_n}{\sum_{n=1}^N z_{k,n}}$.
	\item goto step 2, stop if $\|\mathbf{\red{Z}} - \mathbf{\red{Z}}^\text{new}\|_0 = \|\mathbf{\red{Z}} - \mathbf{\red{Z}}^\text{new}\|^2_F = 0$.
\end{inparaenum}

\section{Gaussian Mixture Models (GMM)}
For GMM let $\red{\boldsymbol{\theta}}_k = (\red{\boldsymbol{\mu}}_k, \red{\boldsymbol{\Sigma}}_k)$; $p_{\theta_k}(\mathbf{\red{x}}) = \mathcal{N}(\mathbf{\red{x}} | \red{\boldsymbol{\mu}}_k, \Sigma_k)$
\begin{compactdesc}
	\item[Mixture Models:] $p_\theta(\mathbf{\red{x}}) = \sum_{k=1}^K \pi_k p_{\theta_k}(\mathbf{\red{x}})$
	\item[Assignment variable (generative model):] $\\z_k \in \{0, 1\}$, $\sum_{k=1}^K z_k = 1$, $\operatorname{Pr}(z_k = 1) = \pi_k \Leftrightarrow p(\mathbf{\red{z}}) = \prod_{k=1}^K \pi_k^{z_k}$
	\item[Complete data distribution:] $p_\theta(\mathbf{\red{x}}, \mathbf{\red{z}}) = \prod_{k=1}^K \left( \red{\boldsymbol{\pi}}_k p_{\theta_k}(\mathbf{\red{x}})\right)^{z_k}$
	\item[Posterior Probabilities:] $\\\operatorname{Pr}(z_k = 1 | \mathbf{\red{x}}) = \frac{\operatorname{Pr}(z_k = 1) p(\mathbf{\red{x}} | z_k = 1)}{\sum_{l=1}^K \operatorname{Pr}(z_l = 1) p(\mathbf{\red{x}} | z_l = 1)} = \frac{\red{\boldsymbol{\pi}}_k p_{\theta_k}(\mathbf{\red{x}})}{\sum_{l=1}^K \red{\boldsymbol{\pi}}_l p_{\theta_l}(\mathbf{\red{x}})}$
	\item[Likelihood of observed data $\mathbf{\red{X}}$:] $p_\theta(\mathbf{\red{X}}) = \prod_{n=1}^N p_\theta(\mathbf{\red{x}}_n) = \prod_{n=1}^N \left(\sum_{k=1}^K \pi_k p_{\theta_k}(\mathbf{\red{x}}_n)\right)$
	\item[Maximize log-likelihood:] $L(\red{\boldsymbol{X}},\red{\boldsymbol{\pi}},\red{\boldsymbol{\mu}},\red{\boldsymbol{\Sigma}}) = ln\ p(\red{\boldsymbol{X}}|\red{\boldsymbol{\pi}},\red{\boldsymbol{\mu}},\red{\boldsymbol{\Sigma}}) = \sum^{N}_{n=1}ln \left\{ \sum^{K}_{k=1} \pi_k \cdot \mathcal{N} \left( \red{\boldsymbol{x}}_n | \red{\boldsymbol{\mu}}_k, \red{\boldsymbol{\Sigma}}_k \right)\right\}$
	\item[MLE:] $\argmax_\theta\sum_{n=1}^N \log \left( \sum_{k=1}^K \pi_k p_{\theta_k}(\mathbf{\red{x}}_n)\right) \break \log \left( \sum_{k=1}^K{\frac{q_k \pi_k p_{\theta_k}(\mathbf{\red{x}}_n)}{q_k}}\right) \ge \sum_{k=1}^K{q_k[\log p_{\theta_k}(\mathbf{\red{x}}_n) + \log \pi_k - \log q_k]}$ with $\sum_{k=1}^K{q_k} = 1$ by Jensen. Lagrangian and get $q_k$ as below.
\end{compactdesc}

\subsection{Expectation-Maximization (EM) for GMM}
\begin{compactenum}
	\item Initialize $\red{\boldsymbol{\pi}}_k^{(0)}, \red{\boldsymbol{\mu}}_k^{(0)}, \red{\boldsymbol{\Sigma}}_k^{(0)}$ for $k = 1, \ldots, K$ and $t=1$.
	\item E-Step: Pr$[z_{k,n} = 1 | \mathbf{\red{x}}_n] = q_{k, n} = \frac{\red{\boldsymbol{\pi}}_k^{(t-1)} \mathcal{N}(\mathbf{\red{x}}_n | \red{\boldsymbol{\mu}}_k^{(t-1)}, \red{\boldsymbol{\Sigma}}_k^{(t-1)})}{\sum_{j=1}^K \red{\boldsymbol{\pi}}_j^{(t-1)} \mathcal{N}(\mathbf{\red{x}}_n | \red{\boldsymbol{\mu}}_j^{(t-1)}, \red{\boldsymbol{\Sigma}}_j^{(t-1)})}$
	\item M-Step: $\red{\boldsymbol{\mu}}_k^{(t)} := \frac{\sum_{n=1}^N q_{k,n} \mathbf{\red{x}}_n}{\sum_{n=1}^N q_{k,n}}$\hspace{20pt} \& \hspace{20pt} $\red{\boldsymbol{\pi}}_k^{(t)} := \frac{1}{N} \sum_{n=1}^N q_{k,n}$ \hspace{20pt} \& \hspace{20pt} $\Sigma_k^{(t)} = \frac{\sum_{n=1}^N q_{k, n} (\mathbf{\red{x}}_n - \red{\boldsymbol{\mu}}_k^{(t)})(\mathbf{\red{x}}_n - \red{\boldsymbol{\mu}}_k^{(t)})^\top}{\sum_{n=1}^N q_{k,n}}$
	\item Repeat from (2.) with $t = t + 1$ if not $\| \log p(\mathbf{\red{X}} | \red{\boldsymbol{\pi}}^{(t)}, \red{\boldsymbol{\mu}}^{(t)}, \red{\boldsymbol{\Sigma}}^{(t)}) - \log p(\mathbf{\red{X}} | \red{\boldsymbol{\pi}}^{(t-1)}, \red{\boldsymbol{\mu}}^{(t-1)}, \red{\boldsymbol{\Sigma}}^{(t-1)}) \| < \epsilon$
\end{compactenum}

\subsection{Model Order Selection (AIC / BIC for GMM)}
Trade-off: data fit (likelihood $p(\mathbf{\red{X}} | \theta)$) vs complexity (\#free parameters $\kappa(\cdot)$). Choosing $K$:
\begin{inparaitem}[\color{bulletcolor}\textbullet]
	\item $\operatorname{AIC}(\theta | \mathbf{\red{X}}) = -\log p_\theta(\mathbf{\red{X}}) + \kappa(\theta)$
	\item $\operatorname{BIC}(\theta | \mathbf{\red{X}}) = -\log p_\theta(\mathbf{\red{X}}) + \frac{1}{2} \kappa(\theta) \log N$
	\item AIC vs BIC for dif. $K$: smaller = better. BIC penalizes complexity more.
\end{inparaitem}

\section{Word Embeddings}
\begin{compactdesc}
  \item[Distributional Model:] $p_\theta(w|w') = $ Pr[$w$ occurs close to $w'$]
  \item[Log-likelihood:] $L(\theta; \mathbf{\red{w}}) = \sum_{t=1}^T\sum_{\Delta \in I}{\log p_\theta(w^{(t+\Delta)}|w^{(t)})}$
  \item[Latent Vector Model:] $w \mapsto (\mathbf{\red{x}}_w, b_w) \in \mathbb{R}^{D+1} \\p_{\theta}(w|w') = \frac{\exp[\langle \mathbf{\red{x}}_w,\mathbf{\red{x}}_{w'}\rangle + b_w]}{\sum_{v\in V}{\exp[\langle \mathbf{\red{x}}_v,\mathbf{\red{x}}_{w'}\rangle + b_v ]}}$.
  \begin{inparaitem}[\color{bulletcolor}\textbullet]
  \item vocab $V$, context vocab $C$: $\log p_{\theta}(w|w') = \langle  y_{w} , x_{w'} \rangle + b_w$,  word embed. $y_w$, context embed. $x_{w'}$
  \item use GloVe objective
  \end{inparaitem}
\end{compactdesc}

\subsection{GloVe (Weighted Square Loss)}
\begin{compactdesc}
  \item[Co-occurence Matrix:]$\mathbf{\red{N}} = (n_{ij}) \in \mathbb{R}^{|V|\cdot |C|} \leftrightarrow \# w_i$ in c'txt $w_j$
  \item[Objective:] $H(\theta;\mathbf{\red{N}}) = \sum_{n_{ij} > 0} f(n_{ij})(\log n_{ij} - \log \exp[\langle \mathbf{\red{x}}_i, \mathbf{\red{y}}_j \rangle + b_i + d_j])^2$ with $f(n) = \min\{1, (\frac{n}{n_{max}})^\alpha\}$, $\alpha \in (0;1]$.
\end{compactdesc}

\begin{compactdesc}
  \item[SGD:] 1. $\mathbf{\red{x}}_i^{new} \leftarrow \mathbf{\red{x}}_i + 2\eta f(n_{ij})(\log n_{ij} - \langle \mathbf{\red{x}}_i, \mathbf{\red{y}}_j \rangle)\mathbf{\red{y}}_j$
  \item \hspace{26pt}2. $\mathbf{\red{y}}_j^{new} \leftarrow \mathbf{\red{y}}_j + 2\eta f(n_{ij})(\log n_{ij} - \langle \mathbf{\red{x}}_i, \mathbf{\red{y}}_j \rangle)\mathbf{\red{x}}_i$
\end{compactdesc}

\section{Non-Negative Matrix Factorization (NMF) / pLSA}
\begin{compactdesc}
	\item[Context Model:] $p(w | d) = \sum_{z=1}^K p(w | z) p(z | d)$
	\item[Conditional independence assumption ($*$):] $p(w|d) = \sum_z p(w,z|d) = \sum_z p(w|d,z)p(z|d) \stackrel{*}{=} \sum_z p(w|z)p(z|d)$
	\item[Symmetric parameterization:] $p(w, d) = \sum_z p(z)p(w | z) p(d | z)$
\end{compactdesc}

\subsection{EM for pLSA:}
$x_{ij} = $ \# occurences of $w_j$ in $d_i$
\begin{compactenum}
  \item Log-Likelihood: $L(\mathbf{\red{U}}, \mathbf{\red{V}}) = \sum_{i,j} x_{i,j}\log p(w_j|d_i) = \break \sum_{(i,j) \in X} \log \sum_{z=1}^K p(w_j|z)p(z|d_i)$
	\item E-Step (optimal q): $q_{zij} = \frac{p(w_j|z)p(z|d_i)}{\sum_{k=1}^K p(w_j|k)p(k|d_i)} := \frac{v_{zj}u_{zi}}{\sum_{k=1}^K u_{kj} v_{ki}}$
	\item M-Steps: $u_{zi} = p(z|d_i) = \frac{\sum_j x_{ij}q_{zij}}{\sum_j x_{ij}}, v_{zj} = p(w_j|z) = \frac{\sum_i x_{ij}q_{zij}}{\sum_{i,l}x_{il}q_{zil}}$
\end{compactenum}

\subsection{NMF Algorithm for quadratic cost function}
\begin{inparaitem}[\color{bulletcolor}\textbullet]
	\item $\mathbf{\red{X}} \in \mathbb{Z}^{N \times M}_{\geq 0}$
	\item NMF: $\mathbf{\red{X}} \approx \mathbf{\red{U^\top V}}, x_{ij} = \sum_z u_{zi}v_{zj} = \langle \mathbf{\red{u}}_i, \mathbf{\red{v}}_j \rangle$
\end{inparaitem}

$ \min_{\mathbf{\red{U}}, \mathbf{\red{V}}} J(\mathbf{\red{U}}, \mathbf{\red{V}}) = \frac{1}{2} \|\mathbf{\red{X}} - \mathbf{\red{U}}^\top\mathbf{\red{V}}\|_F^2$ s.t. $\forall i,j,z~u_{zi},v_{zj} \geq 0 $

\begin{inparaenum}[\color{bulletcolor} 1.]
	\item init: $\mathbf{\red{U}}, \mathbf{\red{V}} = rand()$
	\item repeat for $\mathit{maxIters}$:
	\item update $\mathbf{\red{U}}$: $(\mathbf{\red{VV}}^\top)\mathbf{\red{U}} = \mathbf{\red{VX}}^\top$
	\item project $u_{zi} = \max \{ 0, u_{zi} \}$
	\item update $\mathbf{\red{V}}$: $(\mathbf{\red{UU}}^\top)\mathbf{\red{V}} = \mathbf{\red{UX}}$
	\item project $v_{zj} = \max \{ 0, v_{zj} \}$
\end{inparaenum}

\section{Convolutional Neural Networks}
\textbf{sigmoid}: $s(x) = \frac {1}{1+e^{-x)}}$:$\nabla_x s(x) = s(x)(1-s(x))$
\textbf{Neurons}: $F_\sigma(\mathbf{\red{x}};\mathbf{\red{w}}) = \sigma(w_0 + \sum_{i=1}^M{x_iw_i})$. \textbf{Output}: linear regression; $\mathbf{\red{y}} = \mathbf{\red{W}}^L\mathbf{\red{x}}^{L-1}$, binary classification; $y_1 = \text{P}[Y=1|\mathbf{\red{x}}] = \frac{1}{1 + \exp[-\langle \mathbf{\red{w}}_1^L,\mathbf{\red{x}}^{L-1}\rangle]}$, multiclass; $y_k = \text{P}[Y=k|\mathbf{\red{x}}]= \frac{\exp[\langle \mathbf{\red{w}}_k^L,\mathbf{\red{x}}^{L-1}\rangle]}{\sum_{m=1}^{K}{\exp[\langle \mathbf{\red{w}}_m^L, \mathbf{\red{x}}^{L-1}\rangle]}}$. \textbf{Loss function} $l(y, \hat{y})$: squared loss; $\frac{1}{2}(y - \hat{y})^2$, cross-entropy loss; $-y \log \hat{y} - (1-y)\log(1-\hat{y})$.
Convolution: $F_{n,m}(\mathbf{\red{x}};\mathbf{\red{w}}) = \sigma(b + \sum_{k=-2}^2\sum_{l=-2}^{2}{w_{k,l}x_{n+k,m+l}})$.
\begin{inparaitem}[\color{bulletcolor}\textbullet]
	\item $l$ n-channel filters $K^l$
	\item channel $K^l_c, c\in{[1..n]}$
	\item pixels $(K^l_c)_{i,j}, -k \leq i,j \leq k$
	\item input image $(I_c)_{1\leq{c}\leq{n}}$: $(I_c)_{i,j}$
	\item output conv layer $(i',j')$: $(I \star K^l)_{i',j'} = \sum_{1 \leq c \leq n} \sum_{-k \leq i,j \leq k} (I_c)_{i'+i,j'+j} (K_c^l)_{ij}$
	\item zero padding: $(I_c)_{a,b} = 0$ outside pixel range
	\item non-linearity $ReLU(x) = max(0,x)$ applied per pixel
	\item conv \& ReLU: $ReLU((I \star K^l))_{i',j'}$
	\item per channel max-pooling (3x3) without stride, image pixel (i,j): $\max_{-1 \leq i',j' \leq 1} (ReLU(I \star K^l))_{i+i',j+j'}$
\end{inparaitem}

\section{Optimization}

\subsection{Unconstrained min: $\min f(\mathbf{\red{x}})$ with $\mathbf{\red{x}} \in \mathbb{R}^D$
}

\subsubsection{Coordinate Descent}
\begin{inparaenum}[\color{bulletcolor} 1.]
	\item init: $\mathbf{\red{x}}^{(0)} \in \mathbb{R}^D$
	\item for $t = 0 \ \text{to} \ \mathit{maxIter}$:
	\item sample u.a.r. $d \sim \{1, \ldots, D\}$
	\item $u^\star = \argmin_{u \in \mathbb{R}} f(x_1^{(t)}, .., x_{d-1}^{(t)}, u, x_{d+1}^{(t)}, .., x_D^{(t)})$
	\item $\mathbf{\red{x}}_d^{(t+1)} = u^\star$ and $\mathbf{\red{x}}_i^{(t+1)} = \mathbf{\red{x}}_i^{(t)}$ for $i \neq d$
\end{inparaenum}

\subsubsection{Gradient Descent (or Deepest Descent)}
\textbf{Gradient}: $\nabla f(\mathbf{\red{x}}) := \left( \frac{\partial f(\mathbf{\red{x}})}{\partial \mathbf{\red{x}}_1}, \ldots, \frac{\partial f(\mathbf{\red{x}})}{\partial \mathbf{\red{x}}_D} \right)^\top$
\begin{inparaenum}[\color{bulletcolor} 1.]
	\item init: $\mathbf{\red{x}}^{(0)} \in \mathbb{R}^D$
	\item for $t = 0 \ \text{to} \ \mathit{maxIter}$: $\mathbf{\red{x}}^{(t+1)} = \mathbf{\red{x}}^{(t)} - \gamma \nabla f(\mathbf{\red{x}}^{(t)})$, usually $\gamma \approx \frac{1}{t}$
\end{inparaenum}

\subsubsection{Stochastic Gradient Descent (SGD)}
Assume \textbf{Additive Objective}; $f(x) = \frac{1}{N}\sum_{n=1}^{N}f_n(x)$
\begin{inparaenum}[\color{bulletcolor} 1.]
	\item init: $\mathbf{\red{x}}^{(0)} \in \mathbb{R}^D$
	\item for $t = 0 \ \text{to} \ \mathit{maxIter}$:
	\item sample u.a.r. $n \sim \{1, \ldots, N\}$
	\item $\mathbf{\red{x}}^{(t+1)} = \mathbf{\red{x}}^{(t)} - \gamma \nabla f_n(\mathbf{\red{x}}^{(t)})$, usually stepsize $\gamma \approx \frac{1}{t}$.
\end{inparaenum}

\subsection{Projected Gradient Descent (Constrained Opt.)}
minimize $f(x)$, $x \in Q$ (constraint).
\textbf{Project} $x$ onto $Q$: $P_Q(\mathbf{\red{x}}) = \argmin_{y \in Q} \|\mathbf{\red{y}} - \mathbf{\red{x}}\|$,
\textbf{Projected Gradient Update}: $\mathbf{\red{x}}^{(t+1)} = P_Q[\mathbf{\red{x}}^{(t)} - \gamma \nabla f(\mathbf{\red{x}}^{(t)})]$,
$\mathbf{\red{x}}^{(t+1)}$ is unique if $Q$ convex.

\subsection{Lagrangian Multipliers}
Min $f(\mathbf{\red{x}})$ st $g_i(\mathbf{\red{x}}) \leq 0,\ i:1..m$ \& $h_i(\mathbf{\red{x}}) = \mathbf{\red{a}}_i^\top \mathbf{\red{x}} - b_i = 0,\ i:1..p$
\begin{compactdesc}
	\item[Lagrangian:] $L(\mathbf{\red{x}}, \red{\boldsymbol{\lambda}}, \red{\boldsymbol{\nu}}) := f(\mathbf{\red{x}}) + \sum_{i=1}^m \lambda_i g_i(\mathbf{\red{x}}) + \sum_{i=1}^p \nu_i h_i(\mathbf{\red{x}})$
	\item[Dual function:] $D(\red{\boldsymbol{\lambda}}, \red{\boldsymbol{\nu}}) := \inf_{\mathbf{\red{x}}} L(\mathbf{\red{x}}, \red{\boldsymbol{\lambda}}, \red{\boldsymbol{\nu}}) \in \mathbb{R}$
	\item[Dual Problem:] $\max_{\red{\boldsymbol{\lambda}}, \red{\boldsymbol{\nu}}} D(\red{\boldsymbol{\lambda}}, \red{\boldsymbol{\nu}})$ s.t. $\red{\boldsymbol{\lambda}} \geq \mathbf{\red{0}}$. Note: $\max_{\red{\boldsymbol{\lambda}}, \red{\boldsymbol{\nu}}} D(\red{\boldsymbol{\lambda}}, \red{\boldsymbol{\nu}}) \le \min_\mathbf{\red{x}}{f(\mathbf{\red{x}})}$, equality if $dom\ f$ and $f$ convex
\end{compactdesc}

\subsection{Convex Optimization}
$f : \mathbb{R}^D \rightarrow \mathbb{R}$ is convex, if $dom\ f$ is a convex set, and if $\forall \mathbf{\red{x}}, \mathbf{\red{y}} \in dom\ f$, and for $0 \leq \alpha \leq 1$: $f(\alpha \mathbf{\red{x}} + (1 - \alpha)\mathbf{\red{y}}) \leq \alpha f(\mathbf{\red{x}}) + (1-\alpha)f(\mathbf{\red{y}})$. local=global min, \textbf{Convergence}: $f(\mathbf{\red{x}}^{(t)}) - f(\mathbf{\red{x}}^*) \le \frac{c}{t}$.
\textbf{Subgradient} $g \in \mathbb{R}^D$ of $f$ at $\mathbf{\red{x}}$: $f(\mathbf{\red{y}}) \geq f(\mathbf{\red{x}}) + g^\top(\mathbf{\red{y}}-\mathbf{\red{x}}) \ \forall \mathbf{\red{y}}$
\textbf{Epigraph} of $f:\mathbb{R}^D \rightarrow \mathbb{R}: \left\{ (\mathbf{\red{x}}, t) | \mathbf{\red{x}} \in \mathrm{dom} f, f(x) \le t\right\}$, a fct is convex iff its epigraph is a convex set.
\textbf{Convex fcts} $f(\mathbf{\red{x}}) = \mathbf{\red{a}}^\top \mathbf{\red{x}}$; $f(\mathbf{\red{x}}) = \mathbf{\red{a}}^\top \mathbf{\red{x}} + \mathbf{\red{b}}$; $f(\mathbf{\red{x}}) = e^{\alpha \mathbf{\red{x}}}$; Norms on $\mathbb{R}^D$

\subsubsection{with Equality Constraints}
\begin{inparaitem}[\color{bulletcolor}\textbullet]
\item $\min_{\mathbf{\red{x}}} f(\mathbf{\red{x}})$ s.t. $\mathbf{\red{A}}\mathbf{\red{x}}= \mathbf{\red{b}}$
\item $\mathcal{L}(\mathbf{\red{x}},\red{\boldsymbol{\lambda}}) = f(\mathbf{\red{x}}) + \red{\boldsymbol{\lambda}}^\top (\mathbf{\red{A}}\mathbf{\red{x}}-\mathbf{\red{b}})$
\item $D(\red{\boldsymbol{\lambda}}) = \mathrm{inf}_{\mathbf{\red{x}}} \mathcal{L} (\mathbf{\red{x}}, \red{\boldsymbol{\lambda}})$
\item $\max_{\red{\boldsymbol{\lambda}}} D(\red{\boldsymbol{\lambda}}),\ \red{\boldsymbol{\lambda}}^* \in \arg\max_{\red{\boldsymbol{\lambda}}} D(\red{\boldsymbol{\lambda}})$
\item Recover optimal $\mathbf{\red{x}}^* \in \arg\min_{\mathbf{\red{x}}} \mathcal{L}(\mathbf{\red{x}}, \red{\boldsymbol{\lambda}}^*)$
\end{inparaitem}


\section{Sparse Coding: $\min_{\mathbf{\red{z}}} \|\mathbf{\red{z}}\|_0\ s.t. \|\mathbf{\red{U}}\mathbf{\red{z}}-\mathbf{\red{x}}\|_2 < \sigma $}


Energy preserving: $\|\mathbf{\red{U}}^\top\mathbf{\red{x}}\|^2 = \|\mathbf{\red{x}}\|^2$

\subsection{Orthogonal Basis}
For $\mathbf{\red{x}}$ and o.n.b. $\mathbf{\red{U}}$ compute $\mathbf{\red{z}} = \mathbf{\red{U}}^\top \mathbf{\red{x}} $. Approx $ \mathbf{\red{\hat{x}}} = \mathbf{\red{U\hat{z}}}$, $\hat{z}_i = z_i$ if $ \lvert z_i \rvert > \epsilon$ else 0.
Reconstruction Error $\|\mathbf{\red{x}}-\mathbf{\red{\hat{x}}}\|^2 = \sum_{d\notin\sigma}\langle\mathbf{\red{x}},\mathbf{\red{u}}_d\rangle ^2$.


\subsection{Overcomplete Basis}
$\mathbf{\red{U}} \in \mathbb{R}^{D \times  L}$ and $L > D$. Decoding involved $\rightarrow$ add constraint $\mathbf{\red{z}}^\star \in \arg\min_\mathbf{\red{z}} \lVert \mathbf{\red{z}} \rVert_0$ s.t. $\mathbf{\red{x}} = \mathbf{\red{Uz}}$. NP-hard (non-convex) $\rightarrow$ approximate with 1-norm (convex, $\mathbf{\red{z}}^* = \arg\min_{\mathbf{\red{z}}} \|\mathbf{\red{z}}\|_1\ s.t. \mathbf{\red{x}} = \mathbf{\red{U}}\mathbf{\red{z}} $) or with MP.

\textbf{Coherence}
\begin{inparaitem}[\color{bulletcolor}\textbullet]
	\item $m(\mathbf{\red{U}}) = \max_{i,j:\, i \neq j} | \mathbf{\red{u}}_i^\top \mathbf{\red{u}}_j |$
	\item $m(\mathbf{\red{B}}) = 0$ if $\mathbf{\red{B}}$ orthogonal basis
	\item $m([\mathbf{\red{B}}, \mathbf{\red{u}}]) \geq \frac{1}{\sqrt{D}}$ if atom $\mathbf{\red{u}}$ is added to $\mathbf{\red{B}}$
\end{inparaitem}

\textbf{Noisy observations}: $\mathbf{\red{x}}=\mathbf{\red{U}}\mathbf{\red{z}} + \mathbf{\red{n}}$ $\sim \mathcal{N}\left( \mathbf{\red{0}}, \sigma^2\mathbf{\red{I}} \right)$

\textbf{Matching Pursuit (MP)}
approximation of $\mathbf{\red{x}}$ onto $\mathbf{\red{U}}$, using $K$ entries.
Objective: $\mathbf{\red{z}}^\star \in \argmin_{\mathbf{\red{z}}} \|\mathbf{\red{x}} - \mathbf{\red{Uz}} \|_2$, s.t. $\|\mathbf{\red{z}}\|_0 \leq K$
\begin{inparaenum}[\color{bulletcolor}1.]
	\item init: $z \leftarrow 0, r \leftarrow x$
	\item while $\|\mathbf{\red{z}}\|_0 < K$ do
	\item select atom with smallest angle $i^\star = \argmax_i |\langle \mathbf{\red{u}}_i, \mathbf{\red{r}} \rangle|$
	\item update coefficients: $z_{i^\star} \leftarrow z_{i^\star} + \langle \mathbf{\red{u}}_{i^\star}, \mathbf{\red{r}} \rangle$
	\item update residual: $\mathbf{\red{r}} \leftarrow \mathbf{\red{r}} - \langle \mathbf{\red{u}}_{i^\star}, \mathbf{\red{r}} \rangle \mathbf{\red{u}}_{i^\star}$.
\end{inparaenum}

\textbf{Recovery of MP}: Coherence $m(\mathbf{\red{U}}) = \max_{i\neq j}|\left< \mathbf{\red{u}}_i, \mathbf{\red{u}}_j \right>|$,
exact recovery when: $K<\frac{1}{2}\left( 1+\frac{1}{m(\mathbf{\red{U}})} \right)$

\textbf{Compressive Sensing}
\begin{inparaitem}[\color{bulletcolor}\textbullet]
  \item $\mathbf{\red{x}} \in \mathbb{R}^D$, $K$-sparse in o.n.b. $\mathbf{\red{U}}$. $\mathbf{\red{y}} \in \mathbb{R}^M$ with $y_i = \langle \mathbf{\red{w}}_i, \mathbf{\red{x}}\rangle $: $M$ lin. combinations of signal; $\mathbf{\red{y}} = \mathbf{\red{Wx}} = \mathbf{\red{WUz}} = \mathbf{\red{\theta z}}$, $\theta \in \mathbb{R}^{M \times D}$
  \item Reconstruct $\mathbf{\red{x}} \in \mathbb{R}^D$ from $\mathbf{\red{y}}$; find $\mathbf{\red{z}}^\star \in \argmin_{\mathbf{\red{z}}}\|\mathbf{\red{z}}\|_0$, s.t. $\mathbf{\red{y}} = \mathbf{\red{\theta z}}$ (e.g. with MP). Given $\mathbf{\red{z}}$, reconstruct $\mathbf{\red{x}}$ via $\mathbf{\red{x}} = \mathbf{\red{Uz}}$
\end{inparaitem}

\subsection{Dictionary Learning}
Adapt the dictionary to signal characteristics. Objective: $(\mathbf{\red{U}}^\star, \mathbf{\red{Z}}^\star) \in \argmin_\mathbf{\red{U,Z}} \| \mathbf{\red{X}} - \mathbf{\red{U}} \cdot \mathbf{\red{Z}} \|_F^2$.

\textbf{Matrix Factorization by Iter Greedy Minimization}
\begin{inparaenum}[\color{bulletcolor} 1.]
  \item Coding step(column seperable): $\mathbf{\red{Z}}^{t+1} \in \argmin_\mathbf{\red{Z}} \| \mathbf{\red{X}} - \mathbf{\red{U}}^t \mathbf{\red{Z}} \|_F^2$ subject to $\mathbf{\red{Z}}$ being sparse
  \item Dictionary update step: $\mathbf{\red{U}}^{t+1} \in \argmin_\mathbf{\red{U}} \| \mathbf{\red{X}} - \mathbf{\red{UZ}}^{t+1} \|_F^2$, subject to $\forall l\in [L]:\|\mathbf{\red{u}}_l\|_2 = 1$
\end{inparaenum}

\section{Robust PCA}
Idea: Approx. $\mathbf{\red{X}}$ with $\mathbf{\red{L}}_0 + \mathbf{\red{S}}_0$, $\mathbf{\red{L}}_0$ is low-rank, $\mathbf{\red{S}}_0$ is sparse.
\begin{compactitem}
	\item $\min_{\mathbf{\red{L}},\mathbf{\red{S}}}\mathsf{rank}(\mathbf{\red{L}}) + \mu \lVert \mathbf{\red{S}}\rVert_0$, s. t. $\mathbf{\red{L}} + \mathbf{\red{S}} = \mathbf{\red{X}}$. As non-convex, change to $\min_{\mathbf{\red{L}},\mathbf{\red{S}}} \|\mathbf{\red{L}}\|_\star + \lambda \lVert\mathbf{\red{S}}\rVert_1$ (\emph{not} the same in general)
    \item Perfect reconstruction is \emph{not} possible if $\mathbf{\red{S}}$ is low-rank, $\mathbf{\red{L}}$ is sparse, or $\mathbf{\red{X}}$ is low-rank \textit{and} sparse. Formally coherence: $\|\mathbf{\red{U}}^\top \mathbf{\red{e}}_i\|^2 \leq \frac{\nu r}{n}$, $\|\mathbf{\red{V}}^\top \mathbf{\red{e}}_i\|^2 \leq \frac{\nu r}{n}$, $\|\mathbf{\red{UV}}^\top\|^2_{ij} \leq \frac{\nu r}{n^2}$ : $\mathbf{\red{L}}=\mathbf{\red{U}}\mathbf{\red{D}}\mathbf{\red{V}}^\top$
\end{compactitem}

\subsection{Dual Ascent (Gradient Method for Dual Problem)}
$\red{\boldsymbol{\lambda}}^{t+1} = \red{\boldsymbol{\lambda}}^{t} + \eta \nabla D(\red{\boldsymbol{\lambda}}^t)$,
$ \nabla D (\red{\boldsymbol{\lambda}}) = \mathbf{\red{A}}\mathbf{\red{x}}^*-\mathbf{\red{b}}$ for $\mathbf{\red{x}}^* \in \arg\min_\mathbf{\red{x}} \mathcal{L}(\mathbf{\red{x}},\red{\boldsymbol{\lambda}})$

\textbf{Dual Decomposition}: $f(x)$ separable, $\mathbf{\red{x}} = \left[ \mathbf{\red{x}}_1 \dots \mathbf{\red{x}}_N \right]$

$\min_\mathbf{\red{x}} \left[ f(\mathbf{\red{x}}) := f_1(\mathbf{\red{x}}_1) + \dots + f_N(\mathbf{\red{x}}_N) \right]$
s.t.
$\left[ \mathbf{\red{A}}\mathbf{\red{x}} := \sum_{i=1}^{N} \mathbf{\red{A}}_i \mathbf{\red{x}}_i \right]$

$\Rightarrow \mathcal{L}(\mathbf{\red{x}},\red{\boldsymbol{\lambda}}) = \mathcal{L}_1(\mathbf{\red{x}}_1, \red{\boldsymbol{\lambda}}) + \dots + \mathcal{L}_N(\mathbf{\red{x}}_1, \red{\boldsymbol{\lambda}}) - \red{\boldsymbol{\lambda}}^\top\mathbf{\red{b}}$

$\Rightarrow \mathcal{L}_i(\mathbf{\red{x}}_i,\red{\boldsymbol{\lambda}}) = f_i(\mathbf{\red{x}}_i) + \red{\boldsymbol{\lambda}}^\top \mathbf{\red{A}}_i \mathbf{\red{x}}_i$

\textbf{Dual Decomposition for Dual Ascent}: 
\\$\mathbf{\red{x}}_i^{t+1} := \arg\min_{\mathbf{\red{x}}_i} \mathcal{L}_i(\mathbf{\red{x}}_i,\lambda^t)$; 
$\red{\boldsymbol{\lambda}}^{t+1} := \red{\boldsymbol{\lambda}}^t + \eta^t \left( \sum_{i=1}^{N} \mathbf{\red{A}}_i \mathbf{\red{x}}_i^{t+1} -\mathbf{\red{b}} \right) $




\subsection{Alternating Direction Method of Multipliers (ADMM)}
$\min_{\mathbf{\red{x}}_1, \mathbf{\red{x}}_2} f_1(\mathbf{\red{x}}_1) + f_2(\mathbf{\red{x}}_2)$ s. t. $\mathbf{\red{A}}_1 \mathbf{\red{x}}_1 + \mathbf{\red{A}}_2 \mathbf{\red{x}}_2 = \mathbf{\red{b}}$, $f_1, f_2$ convex
\begin{inparaitem}[\color{bulletcolor}\textbullet]
	\item Augmented Lagrangian: $L_p(\mathbf{\red{x}}_1, \mathbf{\red{x}}_2, \red{\boldsymbol{\nu}}) = f_1(\mathbf{\red{x}}_1) + f_2(\mathbf{\red{x}}_2) + \red{\boldsymbol{\nu}}^\top (\mathbf{\red{A}}_1 \mathbf{\red{x}}_1 + \mathbf{\red{A}}_2 \mathbf{\red{x}}_2 - \mathbf{\red{b}}) + \frac{p}{2}\| \mathbf{\red{A}}_1 \mathbf{\red{x}}_1 + \mathbf{\red{A}}_2 \mathbf{\red{x}}_2 - \mathbf{\red{b}} \|_2^2$
	\item ADMM: $\mathbf{\red{x}}_1^{(t+1)} := \argmin_{\mathbf{\red{x}}_1} L_p(\mathbf{\red{x}}_1, \mathbf{\red{x}}_2^{(t)}, \red{\boldsymbol{\nu}}^{(t)})$, $\mathbf{\red{x}}_2^{(t+1)} := \argmin_{\mathbf{\red{x}}_2} L_p(\mathbf{\red{x}}_1^{(t+1)}, \mathbf{\red{x}}_2, \red{\boldsymbol{\nu}}^{(t)})$, $\red{\boldsymbol{\nu}}^{(t+1)} := \red{\boldsymbol{\nu}}^{(t)} + p(\mathbf{\red{A}}_1 \mathbf{\red{x}}_1^{(t+1)} + \mathbf{\red{A}}_2 \mathbf{\red{x}}_2^{(t+1)} - \mathbf{\red{b}})$
	\item ADDM for RPCA: $f_1(\mathbf{\red{L}}) = \|\mathbf{\red{L}}\|_\star$, $f_2(\mathbf{\red{S}}) = \lambda \| \mathbf{\red{S}} \|_1$, $\mathbf{\red{A}}_1 \mathbf{\red{x}}_1 + \mathbf{\red{A}}_2 \mathbf{\red{x}}_2 = \mathbf{\red{b}} \text{ becomes } \mathbf{\red{L}} + \mathbf{\red{S}} = \mathbf{\red{X}}$, therefore $L_p(\mathbf{\red{L}}, \mathbf{\red{S}}, \red{\boldsymbol{\nu}}) = \|\mathbf{\red{L}}\|_* + \nu \|\mathbf{\red{S}}\|_1 + \left< \red{\boldsymbol{\nu}}, \mathrm{vec}(\mathbf{\red{L}}+\mathbf{\red{S}}-\mathbf{\red{X}}) \right> + \frac{P}{2} \| \mathbf{\red{L}}+ \mathbf{\red{S}} - \mathbf{\red{X}} \|_F^2$, 

\end{inparaitem}

\raggedcolumns
\end{multicols*}
\end{document}